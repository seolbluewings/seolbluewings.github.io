---
layout: post
title:  "Latent Dirichlet Allocation"
date: 2021-03-15
author: seolbluewings
categories: \uppercase{ML}
---

[작성중...]

주로 자연어 처리에서 활용되는 LDA(Latent Dirichlet Allocation, 잠재 디리클레 할당)는 문서의 토픽을 결정하는 확률적 토픽 모델(topic model)이다. 미리 알고 있는 토픽별 단어수 분포를 활용하여 이 문서가 어떠한 토픽을 다루고 있는지 결정 짓는다.

즉, [개미,반도체,공매도]와 같은 단어들이 많이 포함된 뉴스 기사를 경제 카테고리에 [대통령,국회]와 같은 단어들이 많이 포함된 뉴스 기사를 정치 카테고리에 배정하는 과정들을 생각하면 된다.

기본적으로 LDA는 이산형(discrete) 데이터에 대한 확률적 생성 모형이다. LDA는 앞서 언급한 것처럼 문서에서 발견된 단어의 빈도를 바탕으로 문서의 토픽을 결정한다. 이를 위해 우리는 단어의 교환성(exchangeablity)이란 중요한 가정을 하게 된다. 이는 단어의 순서를 무시하더라도 단순히 단어의 빈도만 가지고도 문서의 주제를 표현할 수 있다고 판단하는 것이다. 단어의 유무만 중요할 뿐, 단어의 순서는 중요하지 않다는게 LDA의 가정이다. 따라서 LDA 모형에서는 놀랍게도 "Samsung beats TSMC" 라는 문장과 "TSMC beats Samsung" 이란 문장을 차이가 없다고 간주한다.

논의를 더 이어가기에 앞서 우리는 이후 자주 활용할 용어에 대한 정의를 내릴 필요가 있다.

- 단어(word) : 하나의 독립적인 의미를 가진 글자이며 이산형 데이터의 기본 단위(basis unit)
- 문서(document) : 여러 단어로 구성된 것(sequence of words)
- 토픽(topic) : 단어의 분포로 특징지어지는 잠재 다항변수(latent multinomial variable)

![LDA](https://github.com/seolbluewings/seolbluewings.github.io/blob/master/assets/LDA.png?raw=true){:width="100%" height="100%"}{: .align-center}

위의 이미지는 LDA를 소개하는데 활용되는 가장 대표적인 이미지다. 가장 왼쪽에 있는 박스는 각각이 하나의 토픽과 매칭된다. 하나의 토픽 안에는 각 토픽에서 출현할 가능성이 높은 단어들이 포함되어있다. 첫번째 토픽은 [gene, dna, genetic] 과 같은 단어들이 적혀있는 것을 보니 유전과 관련된 토픽일 것이다. 반면 마지막에 있는 토픽은 [data number computer]인 것을 보아하니 IT와 관련된 토픽일 것이라고 생각할 수 있다.

문서에서는 각 토픽에 포함된 단어들이 관찰되는 빈도를 보여주고 있다. 노란색이 가장 많은 것을 보아하니 이 문서는 유전과 관련된 주제를 이야기하고 있을 것으로 판단된다. 이미지에서 가장 우측에 있는 토픽의 분포를 확인하는 작업은 LDA에서 가장 중요한 과정이다. LDA는 이러한 과정을 반영한 2가지 가정에서 출발한다.

- 각 토픽은 단어들이 혼합되어 생성 된다. (Each topic is a mixture over words)
- 각 문서는 토픽들이 혼합되어 생성 된다. (Each document is a mixture over topics)

#### notation setting

전체 D개의 문서(document)가 있다고 하자. 전체 토픽(topic)의 개수는 T개이며 이 토픽은 총 M개의 단어가 포함된 사전에서 비롯되었다고 생각하자.

d번째 문서에서 사용한 단어들을 벡터로 표현하면 다음과 같을 것이다. 여기서 $$N_{d}$$는 d번째 문서에 포함된 단어의 총 개수를 의미하며, 이 단어들은 앞서 정의내린 사전에 속한 M개의 단어들 중 하나에 해당한다.

$$ w^{(d)} = \left(w_{1}^{(d)},...,w_{n}^{(d)},...,w_{N_{d}}^{(d)} \right)  $$

d번째 문서에서 사용한 단어들의 vector를 형성하기 위해서는 우리는 아래의 그림과 같은 2가지 Matrix 형태의 데이터가 필요하다.

![LDA](https://github.com/seolbluewings/seolbluewings.github.io/blob/master/assets/LDA_1.png?raw=true){:width="70%" height="70%"}{: .align-center}

- $$\phi^{(t)}$$는 토픽 t에 대한 단어들의 M-dimensional 확률 분포이다. 특정 토픽에 대한 가장 가능성 높은 단어들의 리스트를 보여주는 역할을 한다.

- $$\theta^{(d)}$$는 문서 d에 대한 토픽들의 T-dimensional 확률 분포이다. 특정 문서에 대해 각 토픽으로 배정될 확률을 보여주는 역할을 한다.

#### LDA Modeling

문서 d에 있는 모든 단어들을 생성하는 과정은 다음의 이미지와 같다.

![LDA](https://github.com/seolbluewings/seolbluewings.github.io/blob/master/assets/LDA_2.png?raw=true){:width="70%" height="70%"}{: .align-center}

글을 쓰는 과정을 생각 해보자. 우선 글의 주제를 정하고 이후 글을 쓰는 과정에서 어떤 단어들로 글을 구성해나갈지 고민하게 된다. 단어를 생성하는 모델도 이와 같은 절차를 통해 진행된다.

- 문서 d에 대해 T개의 토픽 중 하나를 $$\theta^{(d)}$$의 확률로 선택한다. 여기서 $$z$$는 토픽 할당을 표현하기 위한 latent variable 이다.

$$ z_{n}^{(d)}\vert\theta \sim \text{Categorical}(\theta^{(d)}) $$

- 문서 d에 대한 토픽이 결정된 상황에서 단어의 분포에 기반해 단어를 생성한다.

$$w_{n}^{(d)}\vert z_{n}^{(d)},\phi \sim \text{Categorical}(\phi^{z_{n}^{(d)}})$$

우리의 관심 parameter는 $$\phi^{(t)}$$와 $$\theta^{(d)}$$이며 이 두 parameter에 대한 Prior distribution을 설정해야 한다. 두가지 모두 일반적으로 Dirichlet 분포를 할당한다.

$$
\begin{align}
\phi^{(t)} &\sim \text{Dirichlet}_{M}(\beta) \nonumber \\
\theta^{(d)} &\sim \text{Dirichlet}_{T}(\alpha) \nonumber
\end{align}
$$

Target Posterior Distribution은 $$p(z,\phi,\theta\vert w)$$이며 이는 다음과 같이 Factorization 될 수 있다.

$$
\begin{align}
p(z,\phi,\theta \vert w) &\propto p(z,\phi,\theta,w) \propto p(w\vert z,\phi)p(z\vert\theta)p(\phi)p(\theta) \nonumber \\
&\propto \prod_{d=1}^{D}\prod_{n=1}^{N_{d}}p(w_{n}^{(d)}\vert z_{n}^{(d)},\phi)\prod_{d=1}^{D}\prod_{n=1}^{N_{d}}p(z_{n}^{(d)}\vert\theta)\prod_{t=1}^{T}p(\phi^{(t)})\prod_{d=1}^{D}p(\theta^{(d)}) \nonumber
\end{align}
$$

앞서 우리는 $$\theta,\phi$$가 Dirichlet 분포를 따르는 것으로 Prior를 설정하였다.

먼저 $$p(w_{n}^{(d)}\vert z_{n}^{(d)},\phi)$$ 의 probability distribution 형태를 생각해보자. 이 상황은 문서 d의 토픽이 t로 결정된 상황($$z_{n}^{(d)}=t$$)이다. 이 때 총 M개의 단어 중에서 어떤 단어는 활용되고 어떤 단어는 활용되지 않는다.

토픽 t의 m번째 단어의 확률을 $$\phi_{m}^{(t)}$$로 표현할 수 있다. 이 단어의 출현 여부(Y/N)은 $$(\phi_{m}^{(t)})^{I(w_{n}^{(d)}=m)}$$ 으로 표현 가능하다. 모든 토픽에 대해 단어가 나타날 확률은 다음과 같이 표현이 가능할 것이다.

$$ p(w_{n}^{(d)}\vert z_{n}^{(d)},\phi) = \prod_{t=1}^{T}\left(\prod_{m=1}^{M}(\phi_{m}^{(t)})^{I(w_{n}^{(d)}=m)}\right)^{I(z_{n}^{(d)}=t)}$$

다음으로 $$p(z_{n}^{(d)}\vert\theta)$$ 의 probability distribution 형태를 생각해보자. 문서 d에 대해 각 토픽에 대응하는 확률은 $$\theta_{t}^{(d)}$$ 로 표현 가능하다. 이 토픽이 t번째 토픽에 해당하는지 여부는 $$(\theta^{(d)}_{t})^{I(z_{n}^{(d)}=t)}$$ 로 표현이 가능하다. 전체 T개 topic에 대해서는 다음과 같이 표현이 가능할 것이다.

$$ p(z_{n}^{(d)}\vert\theta) = \prod_{t=1}^{T}(\theta^{(d)}_{t})^{I(z_{n}^{(d)}=t)} $$

따라서 우리가 구하고자 하는 Target Posterior Distribution은 다음과 같은 형태를 가질 것이다.

$$
\begin{align}
p(z,\phi,\theta\vert w) &\propto \prod_{d=1}^{D}\prod_{n=1}^{N_{d}}\prod_{t=1}^{T}\left(\prod_{m=1}^{M}(\phi_{m}^{(t)})^{I(w_{n}^{(d)}=m)}\right)^{I(z_{n}^{(d)}=t)} \nonumber \\
&\prod_{d=1}^{D}\prod_{n=1}^{N_{d}}\prod_{t=1}^{T}(\theta^{(d)}_{t})^{I(z_{n}^{(d)}=t)}\prod_{t=1}^{T}\prod_{m=1}^{M}(\phi_{m}^{(t)})^{\beta-1}\prod_{d=1}^{D}\prod_{t=1}^{T}(\theta_{t}^{(d)})^{\alpha-1} \nonumber
\end{align}
$$

이를 log 형태로 표현하면 조금 더 간결하게 표현가능하기 때문에 log 형태를 취한 것을 이용하기로 하자.

$$
\begin{align}
\log{ p(z,\phi,\theta,w) } &\propto \sum_{d=1}^{D}\sum_{n=1}^{N_{d}}\sum_{t=1}^{T}\sum_{m=1}^{M}I(z_{n}^{(d)}=t)I(w_{n}^{(d)}=m)\log{(\phi_{m}^{(t)})} \nonumber \\
&+ \sum_{d}\sum_{n}\sum_{t}I(z_{n}^{(d)}=t)\log{(\theta_{t}^{(d)})} \nonumber \\
&+ \sum_{t}\sum_{m}(\beta-1)\log{(\phi_{m}^{(t)})}+\sum_{d}\sum_{t}(\alpha-1)\log{(\theta_{t}^{(d)})}
\end{align}
$$

Target Posterior Distribution의 형태를 구했으니 이후의 과정은 Variational Inference 방법을 활용하여 추정한다. (Gibbs Sampler를 활용한 방법도 물론 가능하다)

#### Variational Inference

관심 parameter $$\phi,\theta$$와 latent variable $$z$$ 에 대한 분포를 추정하기 위해 우리는 parameter 간 서로 독립(independent)인 mean-field 가정을 한다. Variational Inference에 관한 설명은 다음의 [링크](https://seolbluewings.github.io/ml/2019/05/26/Variational-Inference.html)를 확인하면 된다.

$$q(\theta,\phi,z) = q_{1}(\phi)q_{2}(\theta)q_{3}(z)$$

우리의 목표는 최적의 $$q^{*}(\theta,\phi,z)$$ 를 발견해내는 것이다. 그 이후 각 Step별로 parameter의 conditional distribution의 형태를 활용하여 paramter를 optimization 시킨다.

$$
\begin{align}
q(\phi,\theta,z) &\propto \prod_{d=1}^{D}\prod_{n=1}^{N_{d}}q_{1}(z_{n}^{(d)})\prod_{t=1}^{T}q_{2}(\phi^{(t)})\prod_{d=1}^{D}q_{3}(\theta^{(d)}) \nonumber \\
z_{n}^{(d)} &\sim \text{Categorical}(\psi^{*}) \nonumber \\
\phi^{(t)} &\sim \text{Dirichlet}_{M}(\beta^{*}) \nonumber \\
\theta^{(d)} &\sim \text{Dirichlet}_{T}(\alpha^{*}) \nonumber
\end{align}
$$

이 Target Posterior를 Mean-Field Assumption을 통해 근사한 $$q(\theta,\phi,z)$$를 가지고 이를 바탕으로 각 parameter에 대한 conditional distribution 형태를 구한 후, 이를 바탕으로 parameter를 최적화 시킨다. 즉, 다음의 단계를 거치게 된다.

$$
q_{1}\vert q_{2},q_{3} &\quad \text{optimize} \psi^{*} \nonumber \\
q_{2}\vert q_{1},q_{3} &\quad \text{optimize} \beta^{*} \nonumber \\
q_{3}\vert q_{1},q_{2} &\quad \text{optimize} \alpha^{*} \nonumber
$$








#### 참조 문헌
1. [SMOTE](https://arxiv.org/pdf/1106.1813.pdf) <br>
2. [Borderline-SMOTE](https://sci2s.ugr.es/keel/keel-dataset/pdfs/2005-Han-LNCS.pdf)
