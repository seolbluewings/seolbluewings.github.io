---
layout: post
title:  "Latent Dirichlet Allocation"
date: 2021-03-15
author: seolbluewings
categories: ML
---

[작성중...]

주로 자연어 처리에서 활용되는 LDA(Latent Dirichlet Allocation, 잠재 디리클레 할당)는 문서의 토픽을 결정하는 확률적 토픽 모델(topic model)이다. 미리 알고 있는 토픽별 단어수 분포를 활용하여 이 문서가 어떠한 토픽을 다루고 있는지 결정 짓는다.

즉, [개미,반도체,공매도]와 같은 단어들이 많이 포함된 뉴스 기사를 경제 카테고리에 [대통령,국회]와 같은 단어들이 많이 포함된 뉴스 기사를 정치 카테고리에 배정하는 과정들을 생각하면 된다.

기본적으로 LDA는 이산형(discrete) 데이터에 대한 확률적 생성 모형이다. LDA는 앞서 언급한 것처럼 문서에서 발견된 단어의 빈도를 바탕으로 문서의 토픽을 결정한다. 이를 위해 우리는 단어의 교환성(exchangeablity)이란 중요한 가정을 하게 된다. 이는 단어의 순서를 무시하더라도 단순히 단어의 빈도만 가지고도 문서의 주제를 표현할 수 있다고 판단하는 것이다. 단어의 유무만 중요할 뿐, 단어의 순서는 중요하지 않다는게 LDA의 가정이다. 따라서 LDA 모형에서는 놀랍게도 "Samsung beats TSMC" 라는 문장과 "TSMC beats Samsung" 이란 문장을 차이가 없다고 간주한다.

논의를 더 이어가기에 앞서 우리는 이후 자주 활용할 용어에 대한 정의를 내릴 필요가 있다.

- 단어(word) : 하나의 독립적인 의미를 가진 글자이며 이산형 데이터의 기본 단위(basis unit)
- 문서(document) : 여러 단어로 구성된 것(sequence of words)
- 토픽(topic) : 단어의 분포로 특징지어지는 잠재 다항변수(latent multinomial variable)

![LDA](https://github.com/seolbluewings/seolbluewings.github.io/blob/master/assets/LDA.png?raw=true){:width="100%" height="100%"}{: .align-center}

위의 이미지는 LDA를 소개하는데 활용되는 가장 대표적인 이미지다. 가장 왼쪽에 있는 박스는 각각이 하나의 토픽과 매칭된다. 하나의 토픽 안에는 각 토픽에서 출현할 가능성이 높은 단어들이 포함되어있다. 첫번째 토픽은 [gene, dna, genetic] 과 같은 단어들이 적혀있는 것을 보니 유전과 관련된 토픽일 것이다. 반면 마지막에 있는 토픽은 [data number computer]인 것을 보아하니 IT와 관련된 토픽일 것이라고 생각할 수 있다.

문서에서는 각 토픽에 포함된 단어들이 관찰되는 빈도를 보여주고 있다. 노란색이 가장 많은 것을 보아하니 이 문서는 유전과 관련된 주제를 이야기하고 있을 것으로 판단된다. 이미지에서 가장 우측에 있는 토픽의 분포를 확인하는 작업은 LDA에서 가장 중요한 과정이다.

글쓰는 과정을 생각 해보자. 우선 글의 주제를 정하고 이후 글을 쓰는 과정에서 어떤 단어들로 글을 구성해나갈지 고민하게 된다. LDA는 이러한 과정을 반영한 2가지 가정에서 출발한다.

- 각 토픽은 단어들이 혼합되어 생성 된다. (Each topic is a mixture over words)
- 각 문서는 토픽들이 혼합되어 생성 된다. (Each document is a mixture over topics)

#### notation setting

전체 D개의 문서(document)가 있다고 하자. 전체 토픽(topic)의 개수는 T개이며 이 토픽은 총 M개의 단어가 포함된 사전에서 비롯되었다고 생각하자.

d번째 문서에서 사용한 단어들을 벡터로 표현하면 다음과 같을 것이다. 여기서 $$N_{d}$$는 d번째 문서에 포함된 단어의 총 개수를 의미하며, 이 단어들은 앞서 정의내린 사전에 속한 M개의 단어들 중 하나에 해당한다.

$$ w^{(d)} = \left(w_{1}^{(d)},...,w_{n}^{(d)},...,w_{N_{d}}^{(d)} \right)  $$

d번째 문서에서 사용한 단어들의 vector를 형성하기 위해서는 우리는 아래의 그림과 같은 2가지 Matrix 형태의 데이터가 필요하다.

![LDA](https://github.com/seolbluewings/seolbluewings.github.io/blob/master/assets/LDA_1.png?raw=true){:width="70%" height="70%"}{: .align-center}

- $$\phi^{(t)}$$는 토픽 t에 대한 단어들의 M-dimensional 확률 분포이다. 특정 토픽에 대한 가장 가능성 높은 단어들의 리스트를 보여주는 역할을 한다.

- $$\theta^{(d)}$$는 문서 d에 대한 토픽들의 T-dimensional 확률 분포이다. 특정 문서에 대해 각 토픽으로 배정될 확률을 보여주는 역할을 한다.

#### LDA Modeling

문서 d에 있는 모든 단어들을 생성하는 과정은 다음의 이미지와 같다.

![LDA](https://github.com/seolbluewings/seolbluewings.github.io/blob/master/assets/LDA_2.png?raw=true){:width="70%" height="70%"}{: .align-center}





#### 참조 문헌
1. [SMOTE](https://arxiv.org/pdf/1106.1813.pdf) <br>
2. [Borderline-SMOTE](https://sci2s.ugr.es/keel/keel-dataset/pdfs/2005-Han-LNCS.pdf)
