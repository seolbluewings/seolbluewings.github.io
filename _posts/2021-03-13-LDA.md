---
layout: post
title:  "Latent Dirichlet Allocation"
date: 2021-03-15
author: seolbluewings
categories: ML
---

주로 자연어 처리에서 활용되는 LDA(Latent Dirichlet Allocation, 잠재 디리클레 할당)는 문서의 토픽을 결정하는 확률적 토픽 모델(topic model)이다. 미리 알고 있는 토픽별 단어수 분포를 활용하여 이 문서가 어떠한 토픽을 다루고 있는지 결정 짓는다.

즉, [개미,반도체,공매도]와 같은 단어들이 많이 포함된 뉴스 기사를 경제 카테고리에 [대통령,국회]와 같은 단어들이 많이 포함된 뉴스 기사를 정치 카테고리에 배정하는 과정들을 생각하면 된다.

기본적으로 LDA는 이산형(discrete) 데이터에 대한 확률적 생성 모형이다. LDA는 앞서 언급한 것처럼 문서에서 발견된 단어의 빈도를 바탕으로 문서의 토픽을 결정한다. 이를 위해 우리는 단어의 교환성(exchangeablity)이란 중요한 가정을 하게 된다. 이는 단어의 순서를 무시하더라도 단순히 단어의 빈도만 가지고도 문서의 주제를 표현할 수 있다고 판단하는 것이다. 단어의 유무만 중요할 뿐, 단어의 순서는 중요하지 않다는게 LDA의 가정이다. 따라서 LDA 모형에서는 놀랍게도 "Samsung beats TSMC" 라는 문장과 "TSMC beats Samsung" 이란 문장을 차이가 없다고 간주한다.

논의를 더 이어가기에 앞서 우리는 이후 자주 활용할 용어에 대한 정의를 내릴 필요가 있다.

- 단어(word) : 하나의 독립적인 의미를 가진 글자이며 이산형 데이터의 기본 단위(basis unit)
- 문서(document) : 여러 단어로 구성된 것(sequence of words)
- 토픽(topic) : 단어의 분포로 특징지어지는 잠재 다항변수(latent multinomial variable)

![ID](https://github.com/seolbluewings/seolbluewings.github.io/blob/master/assets/LDA.png?raw=true){:width="80%" height="80%"}{: .align-center}


#### 참조 문헌
1. [SMOTE](https://arxiv.org/pdf/1106.1813.pdf) <br>
2. [Borderline-SMOTE](https://sci2s.ugr.es/keel/keel-dataset/pdfs/2005-Han-LNCS.pdf)
