---
layout: post
title:  "부스팅(Boosting)"
date: 2020-07-14
author: YoungHwan Seol
categories: Statistics
---

부스팅(Boosting)은 배깅(Bagging)과 마찬가지로 간단하면서도 성능이 높은 앙상블 기법이다. 우리는 편향-분산 트레이드오프 관계에서 확인했듯이 모델로 인해 발생하는 오류를 편향 성분과 분산 성분으로 나눌 수 있었다. Bagging이 분산 성분을 줄여서 모델의 오류를 줄이는 방식이라면, Boosting은 편향 성분을 줄이는 방식이라 할 수 있다.

Boosting이 Bagging과 어떤 면에서 차이가 있는지를 먼저 짚고 넘어가야 한다. 간단하게 표현하자면, Boosting은 모델을 Sequential 하게 생성하여 성능을 높이는 앙상블 기법이고 Bagging은 모델을 Parallel 하게 생성하여 성능을 높이는 앙상블 기법이다.

![CF](https://github.com/seolbluewings/seolbluewings.github.io/blob/master/assets/boosting.PNG?raw=true){:width="70%" height="70%"}{: .center}

그림과 같이 Bagging은 서로 독립적인 모델을 생성하고 이렇게 생성한 여러개의 모델의 평균/투표 방식으로 예측을 진행한다. 이 때, 평균을 구하는 과정에서 분산 성분의 오류를 줄이는 것이라 할 수 있다.

반면, Boosting은 Bagging과 달리 모델 간의 상호연관성이 있다. Boosting은 이전 모델의 학습 결과를 바탕으로 잘못 분류된 데이터를 더 잘 맞추기 위해 잘못 분류된 데이터에 대해 더 높은 가중치를 주게 되고 이를 바탕으로 모델의 편향 성분을 줄여나가는 방식의 학습을 진행한다. 즉, 처음에 활용되는 약한 분류기(weak classifier)를 점차 보완하여 결국 강한 분류기(strong classifier)를 만들어내는 앙상블 기법이라 할 수 있다.

#### AdaBoost

에이다부스트(AdaBoost)는 가장 빈번하게 사용되는 Boosting 알고리즘 중 하나다. AdaBoost 알고리즘이 어떻게 진행되는지 다음의 수식들을 통해 살펴보도록 하자.

2가지 클래스를 분류하는 경우 $$y_{n} \in \{-1,1\} $$ 를 가정하자. 훈련 데이터셋 $$\mathbf{X}=\{x_{1},...,x_{N}\}$$, 타깃 변수 $$\mathbf{Y} = \{y_{1},...,y_{N}\}$$ 형태로 존재한다고 하자. 그리고 각 데이터에 대해 가중치 $$ w_{n} = \frac{1}{N} $$ 를 초기값으로 설정하자.

AdaBoost의 각 단계에서 이 가중치 $$w_{n}$$을 수정하여 모델을 업데이트 한다. 그리고 앞서 언급했던 것처럼 기존 모델에서 틀린 예측값을 기록한 데이터에 대해 가중치를 증가하는 방향으로 업데이트를 진행한다. 모델을 총 M회 업데이트 한다고 생각하자.

1. 가중치 $$w_{n}$$의 초기값을 다음과 같이 $$w_{n}^{(1)}=\frac{1}{N}$$ 로 설정한다.

2. 다음의 오류함수를 최소화시키는 모델 $$f_{m}(\mathbf{X})$$ 를 훈련 데이터에 적용한다.

$$ J_{m} = \sum_{n=1}^{N}w_{n}^{(m)}\mathbb{I}(f_{m}(x_{n}) \neq y_{n}) $$

3. 다음의 값을 계산한다.

$$
\begin{align}
\epsilon_{m} &= \frac{\sum_{n=1}^{N}w_{n}^{(m)}\mathbb{I}(f_{m}(x_{n}) \neq y_{n})}{\sum_{n=1}^{N}w_{n}^{(m)}} \nonumber \\
\alpha_{m} &= \log{\frac{1-\epsilon_{m}}{\epsilon_{m}}} \nonumber
\end{align}
$$

$$\epsilon_{m}$$ 값은 각 모델의 데이터셋에 대한 가중 오류율을 의미하며, 이를 바탕으로 계산하는 $$\alpha_{m}$$ 값은 오류율이 낮은, 즉 더 정확한 모델에 대해 더 큰 값을 가지게 될 것이다.

4. 가중치 $$w_{n}^{(m)}$$ 을 다음과 같이 업데이트 한다.

$$ w_{n}^{(m+1)} = w_{n}^{(m)}\text{exp}\{\alpha_{m}\mathbb{I}(f_{m}(x_{n}) \neq y_{n})\} $$

우리는 이 식을 통해서 가중치 $$w_{n}^{(m)}$$ 값이 잘못 분류된 것으로 판별되는 데이터 포인트에 대해 상승하고, 옳게 분류된 것으로 판별되는 데이터 포인트에 대해서는 변동하지 않음을 확인할 수 있다.

5. 최종적으로 다음과 같이 예측을 진행한다. 이는 각 모델에 대해 서로 다른 가중치를 부여하여 값을 도출하는 방식으로 모든 모델이 평등하게 1표씩 행사하는 Bagging의 방식과는 다르다고 할 수 있다.

$$ F_{M}(\mathbf{X}) = \text{sign}(\sum_{m=1}^{M}\alpha_{m}y_{m}(\mathbf{X})) $$




![CF](https://github.com/seolbluewings/seolbluewings.github.io/blob/master/assets/adaboost.PNG?raw=true){:width="70%" height="70%"}{: .center}

#### Gradient Boosting

#### Stochastic Boosting





