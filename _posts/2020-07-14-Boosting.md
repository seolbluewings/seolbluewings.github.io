---
layout: post
title:  "부스팅(Boosting)"
date: 2020-07-14
author: YoungHwan Seol
categories: Statistics
---

부스팅(Boosting)은 배깅(Bagging)과 마찬가지로 간단하면서도 성능이 높은 앙상블 기법이다. 우리는 편향-분산 트레이드오프 관계에서 확인했듯이 모델로 인해 발생하는 오류를 편향 성분과 분산 성분으로 나눌 수 있었다. Bagging이 분산 성분을 줄여서 모델의 오류를 줄이는 방식이라면, Boosting은 편향 성분을 줄이는 방식이라 할 수 있다.

Boosting이 Bagging과 어떤 면에서 차이가 있는지를 먼저 짚고 넘어가야 한다. 간단하게 표현하자면, Boosting은 모델을 Sequential 하게 생성하여 성능을 높이는 앙상블 기법이고 Bagging은 모델을 Parallel 하게 생성하여 성능을 높이는 앙상블 기법이다.

![CF](https://github.com/seolbluewings/seolbluewings.github.io/blob/master/assets/boosting.PNG?raw=true){:width="70%" height="70%"}{: .center}

그림과 같이 Bagging은 서로 독립적인 모델을 생성하고 이렇게 생성한 여러개의 모델의 평균/투표 방식으로 예측을 진행한다. 이 때, 평균을 구하는 과정에서 분산 성분의 오류를 줄이는 것이라 할 수 있다.

반면, Boosting은 Bagging과 달리 모델 간의 상호연관성이 있다. Boosting은 이전 모델의 학습 결과를 바탕으로 잘못 분류된 데이터를 더 잘 맞추기 위해 잘못 분류된 데이터에 대해 더 높은 가중치를 주게 되고 이를 바탕으로 모델의 편향 성분을 줄여나가는 방식의 학습을 진행한다. 즉, 처음에 활용되는 약한 분류기(weak classifier)를 점차 보완하여 결국 강한 분류기(strong classifier)를 만들어내는 앙상블 기법이라 할 수 있다. 

#### AdaBoost

#### Gradient Boosting

#### Stochastic Boosting





