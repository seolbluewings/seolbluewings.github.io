---
layout: post
title:  "부스팅(Boosting)"
date: 2020-07-14
author: YoungHwan Seol
categories: Statistics
---

부스팅(Boosting)은 배깅(Bagging)과 마찬가지로 간단하면서도 성능이 높은 앙상블 기법이다. 우리는 편향-분산 트레이드오프 관계에서 확인했듯이 모델로 인해 발생하는 오류를 편향 성분과 분산 성분으로 나눌 수 있었다. Bagging이 분산 성분을 줄여서 모델의 오류를 줄이는 방식이라면, Boosting은 편향 성분을 줄이는 방식이라 할 수 있다.

Boosting이 Bagging과 어떤 면에서 차이가 있는지를 먼저 짚고 넘어가야 한다. 간단하게 표현하자면, Boosting은 모델을 Sequential 하게 생성하여 성능을 높이는 앙상블 기법이고 Bagging은 모델을 Parallel 하게 생성하여 성능을 높이는 앙상블 기법이다.

![CF](https://github.com/seolbluewings/seolbluewings.github.io/blob/master/assets/boosting.PNG?raw=true){:width="70%" height="70%"}{: .center}

그림과 같이 Bagging은 서로 독립적인 모델을 생성하고 이렇게 생성한 여러개의 모델의 평균/투표 방식으로 예측을 진행한다. 이 때, 평균을 구하는 과정에서 분산 성분의 오류를 줄이는 것이라 할 수 있다.

반면, Boosting은 Bagging과 달리 모델 간의 상호연관성이 있다. Boosting은 이전 모델의 학습 결과를 바탕으로 잘못 분류된 데이터를 더 잘 맞추기 위해 잘못 분류된 데이터에 대해 더 높은 가중치를 주게 되고 이를 바탕으로 모델의 편향 성분을 줄여나가는 방식의 학습을 진행한다. 즉, 처음에 활용되는 약한 분류기(weak classifier)를 점차 보완하여 결국 강한 분류기(strong classifier)를 만들어내는 앙상블 기법이라 할 수 있다.

#### AdaBoost

에이다부스트(AdaBoost)는 가장 빈번하게 사용되는 Boosting 알고리즘 중 하나다. AdaBoost 알고리즘이 어떻게 진행되는지 다음의 수식들을 통해 살펴보도록 하자.

2가지 클래스를 분류하는 경우 $$y_{n} \in \{-1,1\} $$ 를 가정하자. 훈련 데이터셋 $$\mathbf{X}=\{x_{1},...,x_{N}\}$$, 타깃 변수 $$\mathbf{Y} = \{y_{1},...,y_{N}\}$$ 형태로 존재한다고 하자. 그리고 각 데이터에 대해 가중치 $$ w_{n} = \frac{1}{N} $$ 를 초기값으로 설정하자.

AdaBoost의 각 단계에서 이 가중치 $$w_{n}$$을 수정하여 모델을 업데이트 한다. 그리고 앞서 언급했던 것처럼 기존 모델에서 틀린 예측값을 기록한 데이터에 대해 가중치를 증가하는 방향으로 업데이트를 진행한다. 모델을 총 M회 업데이트 한다고 생각하자. 먼저 가중치 $$w_{n}$$의 초기값을 다음과 같이 $$w_{n}^{(1)}=\frac{1}{N}$$ 로 설정한다.

이후, 다음의 오류함수를 최소화시키는 모델 $$f_{m}(\mathbf{X})$$ 를 훈련 데이터에 적용한다.

$$ J_{m} = \sum_{n=1}^{N}w_{n}^{(m)}\mathbb{I}(f_{m}(x_{n}) \neq y_{n}) $$

다음의 값 $$\epsilon_{m}, \alpha_{m}$$을 계산한다.

$$
\begin{align}
\epsilon_{m} &= \frac{\sum_{n=1}^{N}w_{n}^{(m)}\mathbb{I}(f_{m}(x_{n}) \neq y_{n})}{\sum_{n=1}^{N}w_{n}^{(m)}} \nonumber \\
\alpha_{m} &= \log{\frac{1-\epsilon_{m}}{\epsilon_{m}}} \nonumber
\end{align}
$$

$$\epsilon_{m}$$ 값은 각 모델의 데이터셋에 대한 가중 오류율을 의미하며, 이를 바탕으로 계산하는 $$\alpha_{m}$$ 값은 오류율이 낮은, 즉 더 정확한 모델에 대해 더 큰 값을 가지게 될 것이다.

가중치 $$w_{n}^{(m)}$$ 을 다음과 같이 업데이트 한다.

$$ w_{n}^{(m+1)} = w_{n}^{(m)}\text{exp}\{\alpha_{m}\mathbb{I}(f_{m}(x_{n}) \neq y_{n})\} $$

우리는 이 식을 통해서 가중치 $$w_{n}^{(m)}$$ 값이 잘못 분류된 것으로 판별되는 데이터 포인트에 대해 상승하고, 옳게 분류된 것으로 판별되는 데이터 포인트에 대해서는 변동하지 않음을 확인할 수 있다.

최종적으로 다음과 같이 예측을 진행한다. 이는 각 모델에 대해 서로 다른 가중치를 부여하여 값을 도출하는 방식으로 모든 모델이 평등하게 1표씩 행사하는 Bagging의 방식과는 다르다고 할 수 있다.

$$ F_{M}(\mathbf{X}) = \text{sign}\left(\sum_{m=1}^{M}\alpha_{m}y_{m}(\mathbf{X})\right) $$

AdaBoost를 그림으로 표현하자면 아래의 그림과 같다.

![CF](https://github.com/seolbluewings/seolbluewings.github.io/blob/master/assets/adaboost.PNG?raw=true){:width="70%" height="70%"}{: .center}

검정색 점선은 모델은 $$m$$번째 학습 시, 결정되는 결정 경계(hyperplane)라 할 수 있고 녹색 실선은 $$m$$번째 시행까지 생성된 모델들의 조합이 만들어낸 결정 경계라 할 수 있다. 이 그림에서 각 데이터 포인트에 대한 가중치는 원으로 표현되며 우리는 $$m$$차 학습 시, 잘못 분류된 것으로 간주되는 포인트들의 가중치가 커지는 것을 확인할 수 있다.

#### Gradient Boosting

Gradient Boosting은 분류(classification), 회귀(regression) 문제에 빈번하게 활용되는 머신러닝 기법으로 다른 Boosting 기법처럼 약한 학습기를 앙상블하여 강한 학습기를 만드는 방법이다. 최종 모델 $$F(\mahtbf{X})$$ 는 $$F(\mathbf{X}) = \sum_{m=1}^{M}w_{m}h_{m}(\mathbf{X};\theta_{m})$$ 이며 이는 약한 학습기 $$h_{m}(\mathbf{X};\theta_{m})$$ 의 가중평균으로 만들어낸 것이다. 따라서 최종 모델 $$F(\mahtbf{X})$$의 성능을 향상시키기 위해서는 $$\THETA = \{\theta_{1},\theta_{2},...,\theta_{m}\}$$, $$\omega = \{w_{1},...,w_{m}\}$$ 가 최소화 되어야할 것이다.

loss function $$L$$ 을 아래와 같이 정의하고 $$\THETA,\omega$$ 에 대하여 동시에 최소화시켜야 한다. 여기서 $$N$$은 데이터의 개수다.

$$
\text{argmin}_{\THETA,\omega} \sum_{n=1}^{N}L(y_{i},F(x_{i})) = \text{argmin}_{\THETA,\omega} \sum_{n=1}^{N}L(y_{i},\sum_{m=1}^{M}w_{m}h_{m}(x_{i};\theta_{m}))
$$

그런데 $$\THETA$$의 차원과 $$\omega$$의 차원을 고려하였을 때, 둘을 동시에 최적화시키는 것은 결코 쉽지 않은 일이다. 따라서 한 단계씩 진행하며 최적의 모수값을 찾는 것이 바람직하다.

각 $$m$$ 번째 단계에서 생성하게 되는 모델을 $$f_{m}(\mathbf{X})$$ 라 하자. 그리고 이 $$f_{m}(\mathbf{X})$$ 는 $$f_{m}(\mathbf{X}) = f_{m-1}(\mathbf{X}) + w_{m}h_{m}(\mahtbf{X};\theta_{m})$$ 의 식을 만족한다고 하자. 그리고 각 단계에서 생성되는 모델 $$f_{m}(\mathbf{X})$$ 들이 모여 최종적인 앙상블 모델 $$F(\mathbf{X}) = \sum_{m=1}^{M}f_{m}(\mathbf{X})$$ 가 생성된다고 하자.

각 단계에서의 $$f_{m}(\mathbf{X})$$는 다음의 식에서 $$\theta, \omega$$를 최적화함으로써 구할 수 있다.

$$
\text{argmin}_{\theta,\omega}\sum_{i=1}^{N}L(y_{i},f_{m}(x_{i}))\quad m=1,2,...,M
$$

loss function의 대표적인 예시인 squared loss 를 활용하기로 하자. $$ L(y,f(\mathbf{X}) = (y-f(\mathbf{X}))^{2} $$ 그리고 첫번째 $$m=0$$ 단계에서의 생성 모델은 $$f_{0}(\mathbf{X})=0$$ 이라고 하자.











#### Stochastic Boosting





