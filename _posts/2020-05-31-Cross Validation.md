---
layout: post
title:  "교차 검증(Cross-Validation)"
date: 2020-05-31
author: YoungHwan Seol
categories: Statistics
---

앞선 포스팅에서 우리는 전체 MSE를 낮게하는 것 중에서 분산과 편향의 적절한 균형을 찾아야 한다고 말했다. 그렇다면 우리는 어느 선까지 학습을 해야하는지 결정을 지어야 한다. 그 적정선은 어떻게 결정지을 수 있을까?

#### 학습의 일반화(generalization)

예측을 위한 모델의 경우, 훈련 데이터셋(training data set) 뿐만 아니라 아직 마주하지 못한 새로운 데이터(test data set)에 대해서도 예측이 잘 되어야 한다. 훈련 데이터셋을 모두 활용해서 모델을 만들면, 새로운 데이터에 대해서도 예측 성능이 좋을 것이라고 생각할 수 있으나 실제로는 그렇지 않다.

오히려 생성된 모델이 훈련 데이터에 너무 과도하게 적합된 나머지 새로운 데이터셋(test data set)에 대해 분류 또는 예측이 맞지 않는 과적합(overfitting, 이하 오버피팅) 현상이 발생한다.

오버피팅 현상은 훈련 데이터에 지나치게 학습이 이루어진 모델이 훈련 데이터에 대해서 분류/예측을 잘 수행하지만 새로운 데이터에 대해서는 그러지 못할 수 있다. 오버피팅이 있다면 그 반대 개념인 언더피팅(underfitting)이 있을 것이다. 언더피팅은 모델이 지나치게 단순한 결과, 모델이 데이터의 경향성을 충분히 따라가지 못하는 경우 발생한다.

특히 과적합은 우리가 모델링을 하는 과정에서 주의해야할 사항이다. 훈련 데이터를 가지고 과적합을 시킬 경우, 우리는 굉장히 좋은 모델을 만든 것으로 착각하지만 실제로는 그렇지 않기 때문이다.

우리가 지향하는 바는 적정수준의 피팅이 된 학습이다. 언더피팅과 오버피팅은 아래의 모델 복잡도 그림에서 각각 왼쪽, 오른쪽에 해당된다. 우린 최적의 모델 복잡도를 선택해 최적의 피팅을 고르게 된다.

![CV](https://github.com/seolbluewings/seolbluewings.github.io/blob/master/assets/model_complexity.PNG?raw=true){:width="50%" height="50%"}{: .center}

#### 검증 데이터(Validation Set)

오버피팅을 어떻게 피할 수 있을까? 이 질문은 적정수준의 적합을 어떻게 찾아낼 수 있는가란 질문과 동등하며 어느 선까지 모델을 학습시킬 것인가란 질문과도 동등하다. 이에 대한 대표적인 해결방안은 검증 데이터(Validation Set) 개념을 도입하는 것이다.

앞서 우리는 모든 훈련 데이터를 사용해서 모델을 학습시키는 경우, 훈련 데이터에 대한 오버피팅 현상이 발생하는 것을 언급하였다. 그렇다면, 우리는 모든 데이터를 훈련 데이터로 활용할 것이 아니라 어느 순간에 훈련을 중단시켜야 한다. 이 때, 활용하는 것이 검증 데이터셋(Validation Set)이다.

![CV](https://github.com/seolbluewings/seolbluewings.github.io/blob/master/assets/validationset.PNG?raw=true){:width="50%" height="50%"}{: .center}

그림과 같이 훈련 데이터로 모델을 학습시키면 학습시킬수록 훈련 데이터에 대한 에러(Error)가 줄어든다. 그런데 훈련 데이터를 통해 생성된 모델을 검증 데이터셋에 적용하게 되면, 에러는 줄어들다가 어느 순간 증가하게 된다. 이렇게 검증 데이터셋의 에러가 증가하기 시작하는 변곡점이 과적합이 시작되는 곳으로 여겨질 수 있다. 즉, 훈련 데이터를 가지고 이러한 변곡점을 지난 순간까지 학습을 진행하게 되면, 훈련 데이터에 지나치게 적응한 나머지 처음보는 데이터인 검증 데이터에 대해 자꾸 틀린 예측을 진행하게 된다는 것이다.

우리는 검증 데이터셋을 통해 훈련 중인 모델이 과적합 또는 과소적합인지를 확인하여 최적의 적합을 발견해낸다. 이후, 테스트 데이터(test data set)를 활용하여 훈련 데이터 및 검증 데이터를 활용해 도출해낸 최적의 모델을 활용해 성적을 매기게 된다.

보통 우리에게 주어진 전체 데이터셋의 5~60%를 훈련 데이터셋(train data set)으로 설정하고 검증 데이터, 테스트 데이터 셋을 각 20~25% 로 설정한다. 이처럼 데이터를 6:2:2로 나누게 되는 경우, 모델 훈련에는 전체 데이터의 60%만 사용하게 된다. 검증 및 테스트 데이터를 활용하는 것은 과적합 방지를 위한 방법이지만, 전체 데이터의 60%만을 활용하여 모델을 만든다는 것은 문제발생의 소지가 있다. 전체 데이터 중에서 어떤 데이터가 60% 안에 들어갔느냐에 따라 모형이 민감할 것이기 때문이다. 이러한 문제를 해결할 수 있는 방법이 바로 교차 검증(Cross Validation)이다.

#### K-Fold 교차검증(K-Fold Cross Validation)

K-Fold 교차검증을 진행하기 위해선 데이터셋을 우선 훈련 데이터와 테스트 데이터로 구분 짓는다. 그리고 훈련 데이터셋을 K등분한 후, (K-1)개의 Fold를 훈련 데이터셋으로 활용하고 나머지 1개의 Fold를 검증 데이터셋으로 활용하며 검증 데이터셋에 해당하는 Fold를 rotation 시키는 방법이다. 이를 그림으로 표현하면 아래와 같다.

![CV](https://github.com/seolbluewings/seolbluewings.github.io/blob/master/assets/CV.PNG?raw=true){:width="50%" height="50%"}{: .center}

구체적인 절차를 언급하자면 다음과 같이 진행된다.

1. 훈련 데이터를 K개의 부분집합 $$(\{ \mathcal{D}_{1}, \mathcal{D}_{2}, \cdot\cdot\cdot , \mathcal{D}_{k} \}  )  $$ 로 나눈다.
2. 







