---
layout: post
title:  "교차 검증(Cross-Validation)"
date: 2020-05-31
author: YoungHwan Seol
categories: Statistics
---

앞선 포스팅에서 우리는 전체 MSE를 낮게하는 것 중에서 분산과 편향의 적절한 균형을 찾아야 한다고 말했다. 그렇다면 우리는 어느 선까지 학습을 해야하는지 결정을 지어야 한다. 그 적정선은 어떻게 결정지을 수 있을까?

#### 학습의 일반화(generalization)

예측을 위한 모델의 경우, 훈련 데이터셋(training data set) 뿐만 아니라 아직 마주하지 못한 새로운 데이터(test data set)에 대해서도 예측이 잘 되어야 한다. 훈련 데이터셋을 모두 활용해서 모델을 만들면, 새로운 데이터에 대해서도 예측 성능이 좋을 것이라고 생각할 수 있으나 실제로는 그렇지 않다.

오히려 생성된 모델이 훈련 데이터에 너무 과도하게 적합된 나머지 새로운 데이터셋(test data set)에 대해 분류 또는 예측이 맞지 않는 과적합(overfitting, 이하 오버피팅) 현상이 발생한다.

오버피팅 현상은 훈련 데이터에 지나치게 학습이 이루어진 모델이 훈련 데이터에 대해서 분류/예측을 잘 수행하지만 새로운 데이터에 대해서는 그러지 못할 수 있다. 오버피팅이 있다면 그 반대 개념인 언더피팅(underfitting)이 있을 것이다. 언더피팅은 모델이 지나치게 단순한 결과, 모델이 데이터의 경향성을 충분히 따라가지 못하는 경우 발생한다.

특히 과적합은 우리가 모델링을 하는 과정에서 주의해야할 사항이다. 훈련 데이터를 가지고 과적합을 시킬 경우, 우리는 굉장히 좋은 모델을 만든 것으로 착각하지만 실제로는 그렇지 않기 때문이다.

우리가 지향하는 바는 적정수준의 피팅이 된 학습이다. 언더피팅과 오버피팅은 아래의 모델 복잡도 그림에서 각각 왼쪽, 오른쪽에 해당된다. 우린 최적의 모델 복잡도를 선택해 최적의 피팅을 고르게 된다.

![CV](https://github.com/seolbluewings/seolbluewings.github.io/blob/master/assets/model_complexity.PNG?raw=true){:width="50%" height="50%"}{: .center}

#### 검증 데이터(Validation Set)

오버피팅을 어떻게 피할 수 있을까? 이 질문은 적정수준의 적합을 어떻게 찾아낼 수 있는가란 질문과 동등하며 어느 선까지 모델을 학습시킬 것인가란 질문과도 동등하다. 이에 대한 대표적인 해결방안은 검증 데이터(Validation Set) 개념을 도입하는 것이다.

앞서 우리는 모든 훈련 데이터를 사용해서 모델을 학습시키는 경우, 훈련 데이터에 대한 오버피팅 현상이 발생하는 것을 언급하였다. 그렇다면, 우리는 모든 데이터를 훈련 데이터로 활용할 것이 아니라 어느 순간에 훈련을 중단시켜야 한다. 이 때, 활용하는 것이 검증 데이터셋(Validation Set)이다.





