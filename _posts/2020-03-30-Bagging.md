---
layout: post
title:  "배깅과 랜덤 포레스트(Bagging & Random Forest)"
date: 2020-03-30
author: YoungHwan Seol
categories: Statistics
---


앞선 포스팅에서 언급한 것처럼 의사결정나무는 데이터에 따라 분기 기준이 급격하게 바뀔 수 있는 high-variance 모델이다. Variance가 크다는 것은 다른 데이터셋에서 모델 적합이 일관적이지 못하다는 의미로 이는 의사결정나무가 예측을 위한 모델로는 부적합하다는 것을 의미한다고 할 수 있다.

그래서 앙상블(Ensemble) 방법을 활용하여 예측력을 높이는 액션을 취할 수 있다. 통계학에서 이야기하는 앙상블이라는 단어는 '여러가지 알고리즘을 모아 성능을 향상시키는 것'을 의미한다고 해석할 수 있다. 앙상블 방법을 통해 우리는 보통 더 안정적이고(More Stability) 더 예측력이 높은(Better Accuracy) 모델을 만들어낼 수 있다.


#### 부트스트랩(Bootstrap)

부트스트랩은 학습데이터에서 임의의 복원 추출을 진행하는 것이다. 데이터가 N개 존재한다면, 복원 추출을 통해 N개의 데이터를 가진 집합을 만들어낸다. 복원추출이기 때문에 선택되지 못하는 데이터가 발생할 것이다. 그런 데이터들은 각 부트스트랩 샘플의 OOB(Out of Bag)라고 불린다. OOB는 Validation Set과 같이 성능을 측정하기 위한 용도로 활용될 수 있다.

N개의 전체 데이터에서 N번 데이터를 복원 추출하면, 각 데이터가 나타날 확률은 0.632이며 이에 대한 수학적인 계산은 다음과 같다.

$$
\begin{align}
\text{Pr}(\text{Observation} i \in \text{Bootstrap Sample}) &= 1-\prod_{i=1}^{N}\left(1-\frac{1}{N}\right) \\ \nonumber
&= 1-\left(1-\frac{1}{N}  \right)^{N} \\ \nonumber
&\simeq 1-e^{-1} =0.632 \nonumber
\end{align}
$$

#### 배깅(Bagging)

배깅(Bagging)은 부트스트랩(Bootstrap)하고 합친다(Aggregating)을 합친 말이라고 할 수 있다. 배깅의 과정은 다음의 그림과 같다.

![BA](https://github.com/seolbluewings/seolbluewings.github.io/blob/master/assets/Bagging.PNG?raw=true)

부트스트랩을 통해 생긴 개별적인 데이터셋에 개별적인 모델을 생성하게 되고 따라서 각 모델이 만들어지는 과정은 다른 모델이 만들어지는 과정에 영향을 주지 않는다. 따라서 배깅은 Parallel Ensemble이라 불리기도 한다.

이 글의 초반부에 여러 알고리즘을 모아 성능을 향상시키는 것이 앙상블 방법이라고 언급하였다. 배깅은 여러가지 모델을 만드는 것이기 때문에 배깅을 통해 우리는 더 안정적이고 예측력 높은 모델을 만들 수 있어야할 것이다. 왜 배깅이 개별 의사결정나무보다 더 좋은가? 왜 배깅이 통하는가?

기초통계 시간에 배웠던 사례를 하나 떠올려보자. 각각의 분산이 $$\sigma^{2}$$인 n개의 독립변수 $$Z_{1},Z_{2},...Z_{n}$$이 있을 때, $$\text{Var}(\bar{Z}) = \frac{\sigma^{2}}{n}$$ 이라는 것을 떠올려 보자.

따라서 우리는 관측값들의 평균을 구해 분산을 감소시키는 것을 확인할 수 있다. 배깅도 똑같은 절차를 밟는 것이라 할 수 있다. 통계적 학습 방법론의 분산을 감소시키는 것은 결국 1. 모집단에서 많은 트레이닝셋을 만들어내고 2. 각 트레이닝셋을 통해 분리된 예측 모델을 만들어내 3. 모든 예측의 평균을 내는 것이다. 의사결정나무는 1,0으로 결국 값을 나타내줄 것이기 때문에 이를 투표한다고 표현하기도 한다.

배깅이 통하는 이유를 조금 더 수학적으로 표현하자면 다음과 같을 것이다. 여기서 B는 B개의 부트스트랩 트레이닝 데이터셋을 의미하는 것이다. b번째 부트스트랩 트레이닝셋에서 만든 모델을 $$\hat{f}^{*b}(x)$$ 라고 표현하자. 그렇다면 배깅은 다음과 같이 표현할 수 있다.

$$ \hat{f}_{\text{bag}}(x) = \frac{1}{B}\sum_{b=1}^{B} \hat{f}^{*b}(x)$$

그러나 배깅에도 여전히 한계는 존재한다. 배깅의 과정에서 만들어진 의사결정나무는 분기 기준이 비슷할 것이기 때문에 결국 서로 비슷한 모델일 것이다. 따라서 생성된 모델들 간의 Correlation 값이 높을 것이고 이로 인하여 배깅은 의도만큼 분산을 줄이지 못할 수 있다.

분산이 $$\sigma^{2}$$인 B개의 i.i.d(identically, independently distributed) 확률변수의 평균값의 분산은 $$\frac{\sigma^{2}}{B}$$이다. 그러나 i,d(identically distributed, not independent) 조건에서는 correlation 값이 $$\rho$$일 때, 확률변수 평균값의 분산은 $$ \rho\sigma^{2}+\frac{1-\rho}{B}\sigma^{2} $$ 이다. B의 개수를 높이면 두번째 항의 값이 작아지겠지만, 배깅하는 의사결정나무 모델들간의 Correlation이 양수라면, 평균내는 효과가 감소된다.

#### 랜덤 포레스트(Random Forest)

따라서 랜덤 포레스트의 기반이 되는 아이디어는 생성되는 나무들 사이의 correlation을 줄임으로써 배깅의 분산 감소의 정도를 향상시키는 것이다. 이러한 목표는 의사결정나무를 성장시킬 때, 변수를 랜덤하게 선택함으로써 달성할 수 있다. 그래서 랜덤 포레스트라고 불리는 것이다.

![BA](https://github.com/seolbluewings/seolbluewings.github.io/blob/master/assets/RF.PNG?raw=true)


랜덤 포레스트는 의사결정나무들 간의 상관관계를 없어지게 만드는 과정이다. 배깅과 마찬가지로 부트스트랩을 통해 생성된 데이터를 통해 의사결정나무를 만들지만, 전체 P개의 변수들 중 랜덤하게 M개의 변수를 선택하여 이를 대상으로만 분기 기준을 설정한다. 또 다음 분기기준에서 다시 M개의 변수를 랜덤하게 선택하여 분기한다.

권장하는 M값의 크기는 분류 문제일 때는 $$M=\sqrt{P}$$ 이고 회귀 문제일 때는 $$M=\frac{P}{3}$$ 이다. 분류 문제일 때는 최종적으로 생성된 나무에서 다수의 값을 투표를 통해 결정하게 되고 회귀 문제에서는 평균값으로 최종적인 값을 결정하게 된다.


##### 이 글은 다음의 [교재](https://web.stanford.edu/~hastie/Papers/ESLII.pdf)를 참고하였음을 밝힙니다.










