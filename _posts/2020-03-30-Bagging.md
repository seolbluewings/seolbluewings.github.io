---
layout: post
title:  "배깅과 랜덤 포레스트(Bagging & Random Forest)"
date: 2020-03-30
author: seolbluewings
categories: 앙상블
---

앙상블(Ensemble) 기법은 다수의 기초 학습기를 생성하고 이를 결합하여 학습을 시도하는 것을 의미한다. 앙상블이란 단어를 '여러가지 알고리즘을 모아 성능을 향상시키는 것'이라 이해할 수 있다. 앙상블 기법을 통해서 우리는 보통 더 안정적이고(More Stability) 더 예측력이 높은(Better Accuracy) 모델을 생성해낼 수 있다.

#### 앙상블 개요

![BA](https://github.com/seolbluewings/seolbluewings.github.io/blob/master/assets/Ensemble.png?raw=true){:width="70%" height="70%"}{: .center}

그림처럼 앙상블은 base learner를 생성하고 이를 결합하여 더 예측력이 높은 모델을 만들어낸다. 만약 앙상블 모델 생성 과정에서 모두 동일한 base learner를 생성한다면, 이를 동질적(homogenous)인 앙상블이라 부른다. 의사결정나무들을 모아서 랜덤 포레스트 모델을 만드는 것이 바로 동질적 앙상블인 것이다. 같은 것끼리 모으는 앙상블이 있다면 다른 것끼리 모으는 앙상블도 존재할 것이다. 이를 이질적(heterogenous) 앙상블이라 하며 스태킹(Stacking)이 이 이질적 앙상블의 한 종류라고 할 수 있다. 어쨌든 앙상블 모델은 다수의 base learner를 결합하여 단일 학습기보다 더 좋은 성능을 얻어낸다.

#### 부트스트랩(Bootstrap)

부트스트랩은 학습데이터에서 임의의 복원 추출을 진행하는 것이다. 데이터가 N개 존재한다면, 복원 추출을 통해 N개의 데이터를 가진 집합을 만들어낸다. 복원추출이기 때문에 선택되지 못하는 데이터가 발생할 것이다. 그런 데이터들은 각 부트스트랩 샘플의 OOB(Out of Bag)라고 불린다. OOB는 Validation Set과 같이 성능을 측정하기 위한 용도로 활용될 수 있다.

N개의 전체 데이터에서 N번 데이터를 복원 추출하면, 각 데이터가 나타날 확률은 0.632이며 이에 대한 수학적인 계산은 다음과 같다.

$$
\begin{align}
\text{Pr}(\text{Observation} i \in \text{Bootstrap Sample}) &= 1-\prod_{i=1}^{N}\left(1-\frac{1}{N}\right) \\ \nonumber
&= 1-\left(1-\frac{1}{N}  \right)^{N} \\ \nonumber
&\simeq 1-e^{-1} =0.632 \nonumber
\end{align}
$$

#### 배깅(Bagging)

배깅(Bagging)은 부트스트랩(Bootstrap)하고 합친다(Aggregating)을 합친 말이라고 할 수 있다. 배깅의 과정은 다음의 그림과 같다.

![BA](https://github.com/seolbluewings/seolbluewings.github.io/blob/master/assets/Bagging.PNG?raw=true){:width="70%" height="70%"}{: .center}

부트스트랩을 통해 생긴 개별적인 데이터셋에 개별적인 모델을 생성하게 되고 따라서 각 모델이 만들어지는 과정은 다른 모델이 만들어지는 과정에 영향을 주지 않는다. 따라서 배깅은 Parallel Ensemble이라 불리기도 한다.

이 글의 초반부에 여러 알고리즘을 모아 성능을 향상시키는 것이 앙상블 방법이라고 언급하였다. 배깅은 여러가지 모델을 만드는 것이기 때문에 배깅을 통해 우리는 더 안정적이고 예측력 높은 모델을 만들 수 있어야할 것이다. 왜 배깅이 개별 의사결정나무보다 더 좋은가? 왜 배깅이 통하는가?

기초통계 시간에 배웠던 사례를 하나 떠올려보자. 각각의 분산이 $$\sigma^{2}$$인 n개의 독립변수 $$Z_{1},Z_{2},...Z_{n}$$이 있을 때, $$\text{Var}(\bar{Z}) = \frac{\sigma^{2}}{n}$$ 이라는 것을 떠올려 보자.

따라서 우리는 관측값들의 평균을 구해 분산을 감소시키는 것을 확인할 수 있다. 배깅도 똑같은 절차를 밟는 것이라 할 수 있다. 통계적 학습 방법론의 분산을 감소시키는 것은 결국 1. 모집단에서 많은 트레이닝셋을 만들어내고 2. 각 트레이닝셋을 통해 분리된 예측 모델을 만들어내 3. 모든 예측의 평균을 내는 것이다. 의사결정나무는 1,0으로 결국 값을 나타내줄 것이기 때문에 이를 투표한다고 표현하기도 한다.

배깅이 통하는 이유를 조금 더 수학적으로 표현하자면 다음과 같을 것이다. 여기서 B는 B개의 부트스트랩 트레이닝 데이터셋을 의미하는 것이다. b번째 부트스트랩 트레이닝셋에서 만든 모델을 $$\hat{f}^{*b}(x)$$ 라고 표현하자. 그렇다면 배깅은 다음과 같이 표현할 수 있다.

$$ \hat{f}_{\text{bag}}(x) = \frac{1}{B}\sum_{b=1}^{B} \hat{f}^{*b}(x)$$

그러나 배깅에도 여전히 한계는 존재한다. 배깅의 과정에서 만들어진 의사결정나무는 분기 기준이 비슷할 것이기 때문에 결국 서로 비슷한 모델일 것이다. 따라서 생성된 모델들 간의 Correlation 값이 높을 것이고 이로 인하여 배깅은 의도만큼 분산을 줄이지 못할 수 있다.

분산이 $$\sigma^{2}$$인 B개의 i.i.d(identically, independently distributed) 확률변수의 평균값의 분산은 $$\frac{\sigma^{2}}{B}$$이다. 그러나 i,d(identically distributed, not independent) 조건에서는 correlation 값이 $$\rho$$일 때, 확률변수 평균값의 분산은 $$ \rho\sigma^{2}+\frac{1-\rho}{B}\sigma^{2} $$ 이다. B의 개수를 높이면 두번째 항의 값이 작아지겠지만, 배깅하는 의사결정나무 모델들간의 Correlation이 양수라면, 평균내는 효과가 감소된다.

#### 랜덤 포레스트(Random Forest)

따라서 랜덤 포레스트의 기반이 되는 아이디어는 생성되는 나무들 사이의 correlation을 줄임으로써 배깅의 분산 감소의 정도를 향상시키는 것이다. 이러한 목표는 의사결정나무를 성장시킬 때, 변수를 랜덤하게 선택함으로써 달성할 수 있다. 그래서 랜덤 포레스트라고 불리는 것이다.

![BA](https://github.com/seolbluewings/seolbluewings.github.io/blob/master/assets/RF.PNG?raw=true){:width="70%" height="70%"}{: .center}


랜덤 포레스트는 의사결정나무들 간의 상관관계를 없어지게 만드는 과정이다. 배깅과 마찬가지로 부트스트랩을 통해 생성된 데이터를 통해 의사결정나무를 만들지만, 전체 P개의 변수들 중 랜덤하게 M개의 변수를 선택하여 이를 대상으로만 분기 기준을 설정한다. 또 다음 분기기준에서 다시 M개의 변수를 랜덤하게 선택하여 분기한다.

권장하는 M값의 크기는 분류 문제일 때는 $$M=\sqrt{P}$$ 이고 회귀 문제일 때는 $$M=\frac{P}{3}$$ 이다. 분류 문제일 때는 최종적으로 생성된 나무에서 다수의 값을 투표를 통해 결정하게 되고 회귀 문제에서는 평균값으로 최종적인 값을 결정하게 된다.

#### 참고문헌

1. [ESL](https://web.stanford.edu/~hastie/Papers/ESLII.pdf)
2. [PRML](http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf)
2. [단단한 머신러닝](http://www.yes24.com/Product/Goods/88440860)










