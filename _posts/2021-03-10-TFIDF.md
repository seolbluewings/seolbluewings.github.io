---
layout: post
title:  "TF-IDF"
date: 2021-03-10
author: seolbluewings
categories: ML
---

[작성중...]

TF-IDF 개념은 여러 문서가 존재하는 상황에서 특정 단어가 특정 문서 내에서 얼만큼의 중요도를 갖는지 보여주는 통계적인 수치로 텍스트 마이닝 분석에서 가중치로 빈번하게 활용되는 값이다.

TF-IDF는 문서 내에서 단어의 활용 횟수만을 중요시하는 Bag of Words 가정에서 출발한다. Bag of Words는 말하고자 하는 주제가 단어의 사용 빈도에 의해 결정된다고 바라보는 것에서 시작하여 단어의 등장 횟수만 중요할 뿐 단어의 등장 순서는 중요하지 않다고 본다. 이러한 Bag of Words 가정에서 가장 빈번하게 사용되는 통계량이 바로 TF-IDF 이다.

#### TF-IDF 정의

총 $$D$$개의 문서가 존재한다고 가정 하자. $$D = \{d_{1},...,d_{D}\}$$. D개의 문서에는 총 M개의 단어가 존재한다고 하자. $$W = \{w_{1},...w_{M}\}$$

- TF(Term Frequency) : 특정 문서에서 특정 단어의 등장 횟수

TF는 특정 문서 $$d_{i}$$에서 단어 $$w_{j}$$의 등장 횟수를 의미한다. 단어 $$w_{j}$$가 문서 $$d_{i}$$에서 3번 등장했다면, TF(d_{i},w_{j}) = 3 으로 표현이 가능하다. 앞선 포스팅들 중에서 '분포' 란 단어의 TF를 계산해본다면, TF(Gaussian Mixture Model, 분포) = 23, TF(PCA, 분포) = 1 이다.

- DF(Document Frequency) : 특정 단어가 등장한 문서의 수

DF는 특정 단어 $$w_{j}$$가 등장한 문서의 수를 의미한다. 단어 $$w_{j}$$가 전체 D개의 문서 중에서 $$d_{1},d_{5},d_{9}$$에서만 등장했다면, DF(w_{j}) = 3 이다. DF는 각 문서 내에서 해당 단어가 몇번 등장했는지에 대해서는 신경쓰지 않는 값이며 DF 값이 클수록 많은 문서에서 활용되는 범용적인 단어라고 간주할 수 있다.

- IDF(Inverse Document Frequency)

전체 문서의 수를 특정 단어의 DF 값으로 나눈 후 로그함수를 취한 값이다. 수식으로 표현하면 다음과 같은 형태를 지닌다.

$$ IDF(w_{j}) = \log{\left(\frac{D}{1+DF(w_{j})}\right)} $$

IDF의 수식 형태를 통해서 우리는 IDF의 2가지 특징을 유추해볼 수 있다.





![LDA](https://github.com/seolbluewings/seolbluewings.github.io/blob/master/assets/LDA.png?raw=true){:width="100%" height="100%"}{: .aligncenter}

위의 이미지는 LDA를 소개하는데 활용되는 가장 대표적인 이미지다. 가장 왼쪽에 있는 박스는 각각이 하나의 토픽과 매칭된다. 하나의 토픽 안에는 각 토픽에서 출현할 가능성이 높은 단어들이 포함되어있다. 첫번째 토픽은 [gene, dna, genetic] 과 같은 단어들이 적혀있는 것을 보니 유전과 관련된 토픽일 것이다. 반면 마지막에 있는 토픽은 [data number computer]인 것을 보아하니 IT와 관련된 토픽일 것이라고 생각할 수 있다.

문서에서는 각 토픽에 포함된 단어들이 관찰되는 빈도를 보여주고 있다. 노란색이 가장 많은 것을 보아하니 이 문서는 유전과 관련된 주제를 이야기하고 있을 것으로 판단된다. 이미지에서 가장 우측에 있는 토픽의 분포를 확인하는 작업은 LDA에서 가장 중요한 과정이다. LDA는 이러한 과정을 반영한 2가지 가정에서 출발한다.

- 각 토픽은 단어들이 혼합되어 생성 된다. (Each topic is a mixture over words)
- 각 문서는 토픽들이 혼합되어 생성 된다. (Each document is a mixture over topics)

#### notation setting

전체 D개의 문서(document)가 있다고 하자. 전체 토픽(topic)의 개수는 T개이며 이 토픽은 총 M개의 단어가 포함된 사전에서 비롯되었다고 생각하자.

d번째 문서에서 사용한 단어들을 벡터로 표현하면 다음과 같을 것이다. 여기서 $$N_{d}$$는 d번째 문서에 포함된 단어의 총 개수를 의미하며, 이 단어들은 앞서 정의내린 사전에 속한 M개의 단어들 중 하나에 해당한다.

$$ w^{(d)} = \left(w_{1}^{(d)},...,w_{n}^{(d)},...,w_{N_{d}}^{(d)} \right)  $$

d번째 문서에서 사용한 단어들의 vector를 형성하기 위해서는 우리는 아래의 그림과 같은 2가지 Matrix 형태의 데이터가 필요하다.

![LDA](https://github.com/seolbluewings/seolbluewings.github.io/blob/master/assets/LDA_1.png?raw=true){:width="70%" height="70%"}{: .aligncenter}

- $$\phi^{(t)}$$는 토픽 t에 대한 단어들의 M-dimensional 확률 분포이다. 특정 토픽에 대한 가장 가능성 높은 단어들의 리스트를 보여주는 역할을 한다.

- $$\theta^{(d)}$$는 문서 d에 대한 토픽들의 T-dimensional 확률 분포이다. 특정 문서에 대해 각 토픽으로 배정될 확률을 보여주는 역할을 한다.

#### LDA Modeling

문서 d에 있는 모든 단어들을 생성하는 과정은 다음의 이미지와 같다.

![LDA](https://github.com/seolbluewings/seolbluewings.github.io/blob/master/assets/LDA_2.png?raw=true){:width="70%" height="70%"}{: .aligncenter}

글을 쓰는 과정을 생각 해보자. 우선 글의 주제를 정하고 이후 글을 쓰는 과정에서 어떤 단어들로 글을 구성해나갈지 고민하게 된다. 단어를 생성하는 모델도 이와 같은 절차를 통해 진행된다.

- 문서 d에 대해 T개의 토픽 중 하나를 $$\theta^{(d)}$$의 확률로 선택한다. 여기서 $$z$$는 토픽 할당을 표현하기 위한 latent variable 이다.

$$ z_{n}^{(d)}\vert\theta \sim \text{Categorical}(\theta^{(d)}) $$

- 문서 d에 대한 토픽이 결정된 상황에서 단어의 분포에 기반해 단어를 생성한다.

$$w_{n}^{(d)}\vert z_{n}^{(d)},\phi \sim \text{Categorical}(\phi^{z_{n}^{(d)}})$$

우리의 관심 parameter는 $$\phi^{(t)}$$와 $$\theta^{(d)}$$이며 이 두 parameter에 대한 Prior distribution을 설정해야 한다. 두가지 모두 일반적으로 Dirichlet 분포를 할당한다.

$$
\begin{align}
\phi^{(t)} &\sim \text{Dirichlet}_{M}(\beta) \nonumber \\
\theta^{(d)} &\sim \text{Dirichlet}_{T}(\alpha) \nonumber
\end{align}
$$

Target Posterior Distribution은 $$p(z,\phi,\theta\vert w)$$이며 이는 다음과 같이 Factorization 될 수 있다.

$$
\begin{align}
p(z,\phi,\theta \vert w) &\propto p(z,\phi,\theta,w) \propto p(w\vert z,\phi)p(z\vert\theta)p(\phi)p(\theta) \nonumber \\
&\propto \prod_{d=1}^{D}\prod_{n=1}^{N_{d}}p(w_{n}^{(d)}\vert z_{n}^{(d)},\phi)\prod_{d=1}^{D}\prod_{n=1}^{N_{d}}p(z_{n}^{(d)}\vert\theta)\prod_{t=1}^{T}p(\phi^{(t)})\prod_{d=1}^{D}p(\theta^{(d)}) \nonumber
\end{align}
$$

앞서 우리는 $$\theta,\phi$$가 Dirichlet 분포를 따르는 것으로 Prior를 설정하였다.

먼저 $$p(w_{n}^{(d)}\vert z_{n}^{(d)},\phi)$$ 의 probability distribution 형태를 생각해보자. 이 상황은 문서 d의 토픽이 t로 결정된 상황($$z_{n}^{(d)}=t$$)이다. 이 때 총 M개의 단어 중에서 어떤 단어는 활용되고 어떤 단어는 활용되지 않는다.

토픽 t의 m번째 단어의 확률을 $$\phi_{m}^{(t)}$$로 표현할 수 있다. 이 단어의 출현 여부(Y/N)은 $$(\phi_{m}^{(t)})^{I(w_{n}^{(d)}=m)}$$ 으로 표현 가능하다. 모든 토픽에 대해 단어가 나타날 확률은 다음과 같이 표현이 가능할 것이다.

$$ p(w_{n}^{(d)}\vert z_{n}^{(d)},\phi) = \prod_{t=1}^{T}\left(\prod_{m=1}^{M}(\phi_{m}^{(t)})^{I(w_{n}^{(d)}=m)}\right)^{I(z_{n}^{(d)}=t)}$$

다음으로 $$p(z_{n}^{(d)}\vert\theta)$$ 의 probability distribution 형태를 생각해보자. 문서 d에 대해 각 토픽에 대응하는 확률은 $$\theta_{t}^{(d)}$$ 로 표현 가능하다. 이 토픽이 t번째 토픽에 해당하는지 여부는 $$(\theta^{(d)}_{t})^{I(z_{n}^{(d)}=t)}$$ 로 표현이 가능하다. 전체 T개 topic에 대해서는 다음과 같이 표현이 가능할 것이다.

$$ p(z_{n}^{(d)}\vert\theta) = \prod_{t=1}^{T}(\theta^{(d)}_{t})^{I(z_{n}^{(d)}=t)} $$

따라서 우리가 구하고자 하는 Target Posterior Distribution은 다음과 같은 형태를 가질 것이다.

$$
\begin{align}
p(z,\phi,\theta\vert w) &\propto \prod_{d=1}^{D}\prod_{n=1}^{N_{d}}\prod_{t=1}^{T}\left(\prod_{m=1}^{M}(\phi_{m}^{(t)})^{I(w_{n}^{(d)}=m)}\right)^{I(z_{n}^{(d)}=t)} \nonumber \\
&\prod_{d=1}^{D}\prod_{n=1}^{N_{d}}\prod_{t=1}^{T}(\theta^{(d)}_{t})^{I(z_{n}^{(d)}=t)}\prod_{t=1}^{T}\prod_{m=1}^{M}(\phi_{m}^{(t)})^{\beta-1}\prod_{d=1}^{D}\prod_{t=1}^{T}(\theta_{t}^{(d)})^{\alpha-1} \nonumber
\end{align}
$$

이를 log 형태로 표현하면 조금 더 간결하게 표현가능하기 때문에 log 형태를 취한 것을 이용하기로 하자.

$$
\begin{align}
\log{ p(z,\phi,\theta,w) } &\propto \sum_{d=1}^{D}\sum_{n=1}^{N_{d}}\sum_{t=1}^{T}\sum_{m=1}^{M}I(z_{n}^{(d)}=t)I(w_{n}^{(d)}=m)\log{(\phi_{m}^{(t)})} \nonumber \\
&+ \sum_{d}\sum_{n}\sum_{t}I(z_{n}^{(d)}=t)\log{(\theta_{t}^{(d)})} \nonumber \\
&+ \sum_{t}\sum_{m}(\beta-1)\log{(\phi_{m}^{(t)})}+\sum_{d}\sum_{t}(\alpha-1)\log{(\theta_{t}^{(d)})}
\end{align}
$$

Target Posterior Distribution의 형태를 구했으니 이후의 과정은 Variational Inference 방법을 활용하여 추정한다. (Gibbs Sampler를 활용한 방법도 물론 가능하다)

#### Variational Inference

관심 parameter $$\phi,\theta$$와 latent variable $$z$$ 에 대한 분포를 추정하기 위해 우리는 parameter 간 서로 독립(independent)인 mean-field 가정을 한다. Variational Inference에 관한 설명은 다음의 [링크](https://seolbluewings.github.io/ml/2019/05/26/Variational-Inference.html)를 확인하면 된다.

$$q(\theta,\phi,z) = q_{1}(\phi)q_{2}(\theta)q_{3}(z)$$

우리의 목표는 최적의 $$q^{*}(\theta,\phi,z)$$ 를 발견해내는 것이다. 그 이후 각 Step별로 parameter의 conditional distribution의 형태를 활용하여 paramter를 optimization 시킨다.

$$
\begin{align}
q(\phi,\theta,z) &\propto \prod_{d=1}^{D}\prod_{n=1}^{N_{d}}q_{1}(z_{n}^{(d)})\prod_{t=1}^{T}q_{2}(\phi^{(t)})\prod_{d=1}^{D}q_{3}(\theta^{(d)}) \nonumber \\
z_{n}^{(d)} &\sim \text{Categorical}(\psi^{*}) \nonumber \\
\phi^{(t)} &\sim \text{Dirichlet}_{M}(\beta^{*}) \nonumber \\
\theta^{(d)} &\sim \text{Dirichlet}_{T}(\alpha^{*}) \nonumber
\end{align}
$$

이 Target Posterior를 Mean-Field Assumption을 통해 근사한 $$q(\theta,\phi,z)$$를 가지고 이를 바탕으로 각 parameter에 대한 conditional distribution 형태를 구한 후, 이를 바탕으로 parameter를 최적화 시킨다. 즉, 다음의 단계를 거치게 된다.

$$
\begin{align}
q_{1}\vert q_{2},q_{3} &\quad \text{optimize} \quad \psi^{*} \nonumber \\
q_{2}\vert q_{1},q_{3} &\quad \text{optimize} \quad \beta^{*} \nonumber \\
q_{3}\vert q_{1},q_{2} &\quad \text{optimize} \quad \alpha^{*} \nonumber
\end{align}
$$

- $$q_{1}\vert q_{2},q_{3}$$ 를 통해 parameter 최적화하기

- $$q_{2}\vert q_{1},q_{3}$$ 를 통해 parameter 최적화하기

- $$q_{3}\vert q_{1},q_{2}$$ 를 통해 parameter 최적화하기








#### 참조 문헌
1. [SMOTE](https://arxiv.org/pdf/1106.1813.pdf) <br>
2. [Borderline-SMOTE](https://sci2s.ugr.es/keel/keel-dataset/pdfs/2005-Han-LNCS.pdf)
