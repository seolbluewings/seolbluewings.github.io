---
layout: post
title:  "Principal Component Analysis"
date: 2020-12-18
author: seolbluewings
categories: 차원축소
---

주성분 분석(Principal Component Analysis, 이하 PCA)는 데이터에 대한 정보 손실을 최소화하면서 데이터에 내재된 유의미한 기저변수를 발견해내는 과정에 사용되는 기법이다. PCA는 1. 데이터 차원을 줄이는 과정 2. 데이터의 특징 추출 방법 등에 활용된다.

PCA는 기존 데이터 $$\mathbf{X}$$가 $$D$$차원이라고 했을 때, 이 데이터를 최대한 잘 설명해줄 수 있는 $$M$$차원의 주성분(principal component)을 만들어내는 과정이다. 당연히 $$M \leq D$$ 이다. 즉 기존 $$D$$차원의 데이터를 $$M$$차원으로 줄이는 것이다.

조금 더 구체적으로 말하자면, PCA는 기존 데이터셋을 principal subspace라고 불리는 더 낮은 차원의 공간으로 데이터를 orthogonal projection을 시킴으로써 이루어진다. 이러한 orthogonal projection 과정은 데이터 분산이 최대가 되는 방향으로 진행되어야 한다.

![PCA](https://github.com/seolbluewings/seolbluewings.github.io/blob/master/assets/PCA.png?raw=true){:width="70%" height="70%"}

그런데 보통 우리는 분산이 작은 것이 좋다고 꾸준하게 이야기를 들어왔다. 그런데 왜 PCA에서는 분산이 최대화되는 것이 좋은 것일까? 위의 그림과 같이 데이터가 분포해있다고 가정해자. 데이터의 차원을 줄인다는 것은 데이터를 요약하는 과정이라고 받아들일 수 있다. 어느 위치에서 데이터를 바라보아야 가장 데이터를 잘 요약할 수 있는가를 생각해보면, 그 위치는 바로 데이터의 분산이 최대가 되는 위치일 수 밖에 없다. 위의 그림이라면, 1st Principal Component라고 적혀있는 방향으로 데이터를 바라보아야 이 데이터를 가장 잘 요약할 수 있다.

데이터셋 $$\mathbf{X}=(X_{1},...,X_{D})$$가 다음과 같은 평균과 공분산 행렬을 갖는다고 하자.

$$ \mathbb{E}(\mathbf{X})=\mu \quad \text{Cov}(\mathbf{X})=\Sigma $$

이 공분산행렬 $$\Sigma$$는 $$D \times D$$ 크기의 symmetric matrix이기 때문에 다음과 같이 spectral decomposition을 수행할 수 있다.

$$ \Sigma = \Gamma\Lambda\Gamma^{T} = \sum_{i=1}^{D}\lambda_{i}\gamma_{i}\gamma_{i}^{T} $$

여기서 $$\Lambda$$는 $$\Sigma$$의 eigenvalue인 $$(\lambda_{1},...,\lambda_{D})$$들로 이루어진 대각행렬(diagonal matrix)이며, $$\Gamma = (\gamma_{1},...,\gamma_{D})$$는 각 컬럼이 standardized eigenvector인 orthogonal matrix이다.여기서 공분산행렬의 eigenvalues는 다음과 같이 값이 큰 순서대로 정렬되어 있다고 하자. $$\lambda_{1} \geq \lambda_{2} \geq ... \geq \lambda_{D} \geq 0$$. 모든 eigenvalues가 0이상인 것은 공분산행렬이 positive semi definite 행렬이기 때문이다.

#### Principal Component 생성

Principal Component는 $$\mathbf{X}$$의 linear combination을 통해 생성된다. 데이터셋 $$\mathbf{X}$$를 principal component $$\mathbf{Y}$$로 변환시키는 과정은 아래의 수식과 같다.

$$\mathbf{X} \to \mathbf{Y} = \Gamma^{T}(\mathbf{X}-\mu)$$

따라서 $$\mathbf{X}$$의 $$i$$번째 principal component는 다음과 같이 표현된다. 여기서 $$\gamma_{i}$$는 $$\Gamma$$의 $$i$$번째 컬럼을 의미하며 이를 principal component loading이라 부른다.

$$ y_{i} = \gamma_{i}^{T}(\mathbf{X}-\mu)$$

이를 통해 생성된 principal component $$\mathbf{Y}$$에 대해서는 다음의 특징들이 성립한다.

$$
\begin{align}
\text{1.} & E(y_{i}) = 0 \nonumber \\
\text{2.} & \text{Var}(y_{i})= \lambda_{i} \nonumber \\
\text{3.} & \text{Cov}(y_{i},y_{j}) = 0 \quad i \neq j \nonumber \\
\text{4.} & \text{Var}(y_{1}) \geq \text{Var}(y_{2}) \geq .... \geq Var(y_{D}) \geq 0 \nonumber \\
\text{5.} & \sum_{i=1}^{D}\text{Var}(y_{i}) = \text{tr}(\Sigma) \nonumber \\
\text{6.} & \prod_{i=1}^{D}\text{Var}(y_{i}) = \vert \Sigma \vert \nonumber
\end{align}
$$

여기서 2번 항목이 성립하는 이유는 다음과 같다.

$$
\text{Cov}(\mathbf{Y}) = \Gammaa^{T}\text{Cov}(\mathbf{X})\Gamma = \Gamma^{T}\Gamma\Lambda\Gamma^{T}\Gamma = \Lambda
$$

#### 왜 가장 큰 eigenvalue가 분산을 최대화 하는가?









#### 참조 문헌
1. [PRML](http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf) <br>

2. [단단한 머신러닝](http://www.yes24.com/Product/Goods/88440860) <br>

3. [The Elements of Statistical Learning](https://web.stanford.edu/~hastie/Papers/ESLII.pdf)
