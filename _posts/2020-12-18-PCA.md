---
layout: post
title:  "Principal Component Analysis"
date: 2020-12-18
author: seolbluewings
categories: 차원축소
---

주성분 분석(Principal Component Analysis, 이하 PCA)는 데이터에 대한 정보 손실을 최소화하면서 데이터에 내재된 유의미한 기저변수를 발견해내는 과정에 사용되는 기법이다. PCA는 1. 데이터 차원을 줄이는 과정 2. 데이터의 특징 추출 방법 등에 활용된다.

PCA는 기존 데이터 $$\mathbf{X}$$가 $$D$$차원이라고 했을 때, 이 데이터를 최대한 잘 설명해줄 수 있는 $$M$$차원의 주성분(principal component)을 만들어내는 과정이다. 당연히 $$M \leq D$$ 이다. 즉 기존 $$D$$차원의 데이터를 $$M$$차원으로 줄이는 것이다.

조금 더 구체적으로 말하자면, PCA는 기존 데이터셋을 principal subspace라고 불리는 더 낮은 차원의 공간으로 데이터를 orthogonal projection을 시킴으로써 이루어진다. 이러한 orthogonal projection 과정은 데이터 분산이 최대가 되는 방향으로 진행되어야 한다.

![PCA](https://github.com/seolbluewings/seolbluewings.github.io/blob/master/assets/PCA.png?raw=true){:width="100%" height="70%"}

그런데 보통 우리는 분산이 작은 것이 좋다고 꾸준하게 이야기를 들어왔다. 그런데 왜 PCA에서는 분산이 최대화되는 것이 좋은 것일까? 위의 그림과 같이 데이터가 분포해있다고 가정해자. 데이터의 차원을 줄인다는 것은 데이터를 요약하는 과정이라고 받아들일 수 있다. 어느 위치에서 데이터를 바라보아야 가장 데이터를 잘 요약할 수 있는가를 생각해보면, 그 위치는 바로 데이터의 분산이 최대가 되는 위치일 수 밖에 없다. 위의 그림이라면, 1st Principal Component라고 적혀있는 방향으로 데이터를 바라보아야 이 데이터를 가장 잘 요약할 수 있다.

데이터셋 $$\mathbf{X}=(X_{1},...,X_{D})$$가 다음과 같은 평균과 공분산 행렬을 갖는다고 하자.

$$ \mathcal{E}(\mathbf{X})=\mu \quad \text{Cov}(\mathbf{X})=\Sigma $$

이 공분산행렬 $$\Sigma$$는 $$D \times D$$ 크기의 symmetric matrix이기 때문에 다음과 같이 spectral decomposition을 수행할 수 있다.

$$ \Sigma = \Gamma\Lambda\Gamma^{T} = \sum_{i=1}^{D}\lambda_{i}\gamma_{i}\gamma_{i}^{T} $$









#### 참조 문헌
1. [PRML](http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf) <br>

2. [단단한 머신러닝](http://www.yes24.com/Product/Goods/88440860) <br>

3. [The Elements of Statistical Learning](https://web.stanford.edu/~hastie/Papers/ESLII.pdf)
