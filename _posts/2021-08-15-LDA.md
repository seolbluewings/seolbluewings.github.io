---
layout: post
title:  "Latent Dirichlet Allocation(2)"
date: 2021-08-15
author: seolbluewings
categories: Statistics
---

[작성중...]

앞서 LDA 모델에 대해 설명했던 포스팅에 이어 이번 포스팅에서는 LDA 모델에 대한 parameter 추정 방법에 대해 이야기하고자 한다. LDA는 Gaussian Mixture 모델과 마찬가지로 latent variable을 이용해서 문서의 토픽(topic)을 결정짓는다. 토픽이 곧 하나의 군집(Cluster)와 같다고 보면 된다.

그림과 같이 LDA 모델은 3가지 parameter에 대한 값을 구해야한다. 따라서 Target Posterior Distribution은 3가지 paarameter $$(z,\phi,\theta)$$ 에 대한 내용을 담고 있어야할 것이다.

![LDA](https://github.com/seolbluewings/seolbluewings.github.io/blob/master/assets/LDA_2.png?raw=true){:width="70%" height="70%"}{: .aligncenter}

앞선 포스팅에서 정리한 Target Posterior Distribution에 대한 Factorization 결과를 가져온다.

$$
\begin{align}
p(z,\phi,\theta \vert w) &\propto p(z,\phi,\theta,w) \propto p(w\vert z,\phi)p(z\vert\theta)p(\phi)p(\theta) \nonumber \\
&\propto \prod_{d=1}^{D}\prod_{n=1}^{N_{d}}p(w_{n}^{(d)}\vert z_{n}^{(d)},\phi)\prod_{d=1}^{D}\prod_{n=1}^{N_{d}}p(z_{n}^{(d)}\vert\theta)\prod_{t=1}^{T}p(\phi^{(t)})\prod_{d=1}^{D}p(\theta^{(d)}) \nonumber \\
&\propto \prod_{d=1}^{D}\prod_{n=1}^{N_{d}}\prod_{t=1}^{T}\left(\prod_{m=1}^{M}(\phi_{m}^{(t)})^{I(w_{n}^{(d)}=m)}\right)^{I(z_{n}^{(d)}=t)} \nonumber \\
&\quad\prod_{d=1}^{D}\prod_{n=1}^{N_{d}}\prod_{t=1}^{T}(\theta^{(d)}_{t})^{I(z_{n}^{(d)}=t)}\prod_{t=1}^{T}\prod_{m=1}^{M}(\phi_{m}^{(t)})^{\beta-1}\prod_{d=1}^{D}\prod_{t=1}^{T}(\theta_{t}^{(d)})^{\alpha-1} \nonumber
\end{align}
$$

Target Posterior Distribution에 대한 parameter 추정을 위한 방법이 대표적으로 2가지 존재한다. 하나는 Gibbs Sampler이고 다른 하나는 Variational Inference이다. 이번 포스팅에서는 Gibbs Sampler, 그 중에서도 특히 Collapsed Gibbs Sampler를 이용해 parameter에 대한 추정을 진행해보도록 하겠다.

#### Concept of Collapsed Gibbs Sampler

Full-Conditional Gibbs Sampler 보다 효율적인 Gibbs Sampler를 진행하기 위해 Collapsing에 대해 알아두면 좋다. Sampling을 목표하는 parameter가 $$(x_{1},x_{2},x_{3})$$ 라면 다음의 Step으로 Gibbs Sampler를 진행했다.

$$ p(x_{1}\vert x_{2},x{3})\quad\rightarrow\quad p(x_{2}\vert x_{1},x_{3})\quad\rightarrow\quad p(x_{3}\vert x_{1},x_{2}) $$

Collasped Gibbs Sampler는 1,3번째 Sampling Step을 $$x_{2}$$에 대해 Marginalized하여 Sampling을 간결하게 바꾸어준다. 혹은 Marginalized의 결과 $$x_{2}$$에 대한 Sampling 자체가 필요 없어지는 상황이 발생할 수도 있게 된다.

$$ p(x_{1}\vert x{3})\quad\rightarrow\quad p(x_{2}\vert x_{1},x_{3})\quad\rightarrow\quad p(x_{3}\vert x_{1}) $$

LDA에서는 Collapsed Gibbs Sampler를 수식을 간결하게 만들기 위해 사용한다.

#### Collapsed Gibbs Sampler for LDA

Gibbs Sampler 수행을 위해 가장 먼저 Target Distribution $$p(z,\phi,\theta \vert w)$$ 에 대한 Factorization을 수행한다.

$$ p(\mathbf{z},\phi,\theta \vert \mathbf{w},\alpha,\beta) \propto \prod_{d=1}^{D}\prod_{n=1}^{N_{d}}p(w_{n}^{(d)}\vert z_{n}^{(d)},\phi^{(t)})\prod_{d=1}^{D}\prod_{n=1}^{N_{d}}p(z_{n}^{(d)}\vert\theta^{(d)})\prod_{t=1}^{T}p(\phi^{(t)}\vert \beta)\prod_{d=1}^{D}p(\theta^{(d)}\vert \alpha) $$

수식에서 $$\alpha,\beta$$는 Prior로 부여되는 값이고 단어 $$\mathbf{w}$$는 관측치이다. 여기서 $$\mathbf{\theta},\mathbf{\phi}$$에 대한 Marginalized를 시도하면 수식을 보다 간단하게 변형시킬 수 있으며, 이 결과 Collapsed Gibbs Sampler를 수행할 수 있게 된다.

$$\mathbf{\theta},\mathbf{\phi}$$ 를 Marginalized한 수식은 다음과 같이 구할 수 있을 것이다.

$$ p(\mathbf{z} \vert \mathbf{w} \mathbf{\alpha},\mathbf{\beta}) = \int_{\mathbf{\theta}}\int_{\mathbf{\phi}}p(\mathbf{z},\mathbf{\phi},\mathbf{\theta}\vert \mathbf{w},\mathbf{\alpha},\mathbf{\beta})d\theta d\phi $$

이 수식에서 적분 기호 두의 수식에 Factorization 결과를 넣고 $$\theta$$ 파트와 $$\phi$$ 파트로 수식을 분리한다.

$$
p(\mathbf{z} \vert \mathbf{w} \mathbf{\alpha},\mathbf{\beta}) = \int_{\phi} \prod_{t=1}^{T}p(\phi^{(t)}\vert \beta) \prod_{d=1}^{D}\prod_{n=1}^{N_{d}}p(w_{n}^{(d)}\vert z_{n}^{(d)},\phi^{(t)})d\phi \int_{\theta} \prod_{d=1}^{D}p(\theta^{(d)}\vert \alpha) \prod_{d=1}^{D}\prod_{n=1}^{N_{d}}p(z_{n}^{(d)}\vert\theta^{(d)})d\theta
$$

이제 이 수식에서 적분을 직접 계산하지 않고 수식을 간단히 변형시킬 수 있는 방법을 고민해보자.

먼저 $$\phi$$ 파트에 대한 적분을 처리하면 다음과 같다.

$$ \int_{\phi} \prod_{t=1}^{T}p(\phi^{(t)}\vert \beta) \prod_{d=1}^{D}\prod_{n=1}^{N_{d}}p(w_{n}^{(d)}\vert z_{n}^{(d)},\phi^{(t)})d\phi = \prod_{t=1}^{T}\int_{\phi^{(t)}} \frac{\Gamma\left(\sum_{m=1}^{M}\beta_{m}  \right)}{\prod_{m=1}^{M}\Gamma(\beta_{m})}\prod_{m=1}^{M}\left[\phi_{m}^{(t)}\right]^{\beta_{m}-1} \prod_{m=1}^{M}\left[\phi_{m}^{(t)}\right]^{\sum_{d=1}^{D}\sum_{n=1}^{N_{d} I(w_{n}^{(d)}=n)I(z_{n}^{(d)}=t)}d\phi^{(t)} $$

이 수식에서 $$\phi_{m}^{(t)}$$ 는 m번재 단어가 토픽 t에 할당된 횟수를 문서의 종류$$(1,...,D)$$ 관계 없이 집계하며 이러한 의도를 담은 수식은 $$ \sum_{d=1}^{D}\sum_{n=1}^{N_{d} I(w_{n}^{(d)}=n)I(z_{n}^{(d)}=t) $$ 이다.

수식을 잘 살펴보면, $$\phi_{m}^{(t)}$$ 에 대하여 다시 Dirichlet 형태의 Form으로 수식을 수정하는 것이 가능해보인다.

$$
\begin{align}
\prod_{t=1}^{T}\int_{\phi^{(t)}} \frac{\Gamma\left(\sum_{m=1}^{M}\beta_{m}  \right)}{\prod_{m=1}^{M}\Gamma(\beta_{m})}\prod_{m=1}^{M}\left[\phi_{m}^{(t)}\right]^{\beta_{m}-1} \prod_{m=1}^{M}\left[\phi_{m}^{(t)}\right]^{\sum_{d=1}^{D}\sum_{n=1}^{N_{d} I(w_{n}^{(d)}=n)I(z_{n}^{(d)}=t)}d\phi^{(t)} &=
\prod_{t=1}^{T}\frac{\Gamma\left(\sum_{m=1}^{M}\beta_{m}\right) \prod_{m=1}^{M}\Gamma\left(\sum_{m=1}^{M}  \beta_{m}+ \sum_{d=1}^{D}\sum_{n=1}^{N_{d}}I(w_{n}^{(d)}=m)I(z_{n}^{(d)}=t) \right)}{  \prod_{m=1}^{M}\Gamma(\beta_{m})\Gamma\left( \sum_{m=1}^{M}  \beta_{m}+ \sum_{d=1}^{D}\sum_{n=1}^{N_{d}}I(w_{n}^{(d)}=m)I(z_{n}^{(d)}=t) \right)  } \int_{\phi^{(t)}} \frac{\Gamma\left( \sum_{m=1}^{M}  \beta_{m}+ \sum_{d=1}^{D}\sum_{n=1}^{N_{d}}I(w_{n}^{(d)}=m)I(z_{n}^{(d)}=t) \right) }{\prod_{m=1}^{M}\Gamma\left(\sum_{m=1}^{M}  \beta_{m}+ \sum_{d=1}^{D}\sum_{n=1}^{N_{d}}I(w_{n}^{(d)}=m)I(z_{n}^{(d)}=t) \right)} \left[\phi_{m}^{(t)} \right]^{\beta_{m}+\sum_{d=1}^{D}\sum_{n=1}^{N_{d} I(w_{n}^{(d)}=n)I(z_{n}^{(d)}=t)-1} \nonumber \\
&= \prod_{t=1}^{T}\frac{\Gamma\left(\sum_{m=1}^{M}\beta_{m}\right) \prod_{m=1}^{M}\Gamma\left(\sum_{m=1}^{M}  \beta_{m}+ \sum_{d=1}^{D}\sum_{n=1}^{N_{d}}I(w_{n}^{(d)}=m)I(z_{n}^{(d)}=t) \right)}{  \prod_{m=1}^{M}\Gamma(\beta_{m})\Gamma\left( \sum_{m=1}^{M}  \beta_{m}+ \sum_{d=1}^{D}\sum_{n=1}^{N_{d}}I(w_{n}^{(d)}=m)I(z_{n}^{(d)}=t) \right)  } \nonumber
\end{align}
$$





(작성 예정)......




#### 참조 문헌
1. [Understand the LDA](https://www.edwith.org/machinelearning2__17/lecture/10882?isDesc=false)
2. [단단한 머신러닝](http://www.yes24.com/Product/Goods/88440860)
3. 
