---
layout: post
title:  "Latent Dirichlet Allocation(2)"
date: 2021-08-15
author: seolbluewings
categories: Statistics
---

[작성중...]

앞서 LDA 모델에 대해 설명했던 포스팅에 이어 이번 포스팅에서는 LDA 모델에 대한 parameter 추정 방법에 대해 이야기하고자 한다. LDA는 Gaussian Mixture 모델과 마찬가지로 latent variable을 이용해서 문서의 토픽(topic)을 결정짓는다. 토픽이 곧 하나의 군집(Cluster)와 같다고 보면 된다.

그림과 같이 LDA 모델은 3가지 parameter에 대한 값을 구해야한다. 따라서 Target Posterior Distribution은 3가지 paarameter $$(z,\phi,\theta)$$ 에 대한 내용을 담고 있어야할 것이다.

![LDA](https://github.com/seolbluewings/seolbluewings.github.io/blob/master/assets/LDA_2.png?raw=true){:width="70%" height="70%"}{: .aligncenter}

앞선 포스팅에서 정리한 Target Posterior Distribution에 대한 Factorization 결과를 가져온다.

$$
\begin{align}
p(z,\phi,\theta \vert w) &\propto p(z,\phi,\theta,w) \propto p(w\vert z,\phi)p(z\vert\theta)p(\phi)p(\theta) \nonumber \\
&\propto \prod_{d=1}^{D}\prod_{n=1}^{N_{d}}p(w_{n}^{(d)}\vert z_{n}^{(d)},\phi)\prod_{d=1}^{D}\prod_{n=1}^{N_{d}}p(z_{n}^{(d)}\vert\theta)\prod_{t=1}^{T}p(\phi^{(t)})\prod_{d=1}^{D}p(\theta^{(d)}) \nonumber \\
&\propto \prod_{d=1}^{D}\prod_{n=1}^{N_{d}}\prod_{t=1}^{T}\left(\prod_{m=1}^{M}(\phi_{m}^{(t)})^{I(w_{n}^{(d)}=m)}\right)^{I(z_{n}^{(d)}=t)} \nonumber \\
&\prod_{d=1}^{D}\prod_{n=1}^{N_{d}}\prod_{t=1}^{T}(\theta^{(d)}_{t})^{I(z_{n}^{(d)}=t)}\prod_{t=1}^{T}\prod_{m=1}^{M}(\phi_{m}^{(t)})^{\beta-1}\prod_{d=1}^{D}\prod_{t=1}^{T}(\theta_{t}^{(d)})^{\alpha-1} \nonumber
\end{align}
$$

Target Posterior Distribution에 대한 parameter 추정을 위한 방법이 대표적으로 2가지 존재한다. 하나는 Gibbs Sampler이고 다른 하나는 Variational Inference이다. 이번 포스팅에서는 Gibbs Sampler, 그 중에서도 특히 Collapsed Gibbs Sampler를 이용해 parameter에 대한 추정을 진행해보도록 하겠다.

(작성 예정)......




#### 참조 문헌
1. [Understand the LDA](https://www.edwith.org/machinelearning2__17/lecture/10882?isDesc=false)
2. [단단한 머신러닝](http://www.yes24.com/Product/Goods/88440860)
3. 
